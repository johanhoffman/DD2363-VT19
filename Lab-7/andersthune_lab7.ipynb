{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Note:** This notebook is written in the Julia language, so the cells\n",
    "  can't be executed in Google Colab. If you want to verify that the\n",
    "  notebook works, I recommend [JuliaBox](https://juliabox.com/) (which\n",
    "  is free and requires no installation) or testing locally. The syntax\n",
    "  is very similar to Python and MATLAB. Note in particular the dot\n",
    "  syntax used to perform elementwise operations ( `f.(x)` applies `f`\n",
    "  to all elements of `x` ), that indices start at 1 and that the last\n",
    "  statement of all functions is returned automatically.\n",
    "\n",
    "**Extra note:** The creators of Julia recently received the Wilkinson\n",
    "  Prize for Numerical Software, a prize also awarded to the creator of\n",
    "  the Triangle mesh generator. Here is a\n",
    "  [talk](https://www.youtube.com/watch?v=OfMP5PTFQk0&feature=youtu.be)\n",
    "  from the event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "6RgtXlfYO_i7",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Lab 7: Optimization and learning**\n",
    "**Anders Ågren Thuné**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "9x_J5FVuPzbm",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "6UFTSzW7P8kL",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "OkT8J7uOWpT3",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **About the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "id": "Pdll1Xc9WP0e",
    "outputId": "1e1de3c9-fbe5-46c9-c3de-2986013df3c9",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DD2363 Methods in Scientific Computing,\\nKTH Royal Institute of Technology, Stockholm, Sweden.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DD2363 Methods in Scientific Computing,\n",
    "KTH Royal Institute of Technology, Stockholm, Sweden.\n",
    "\"\"\"\n",
    "\n",
    "# Copyright (C) 2019\n",
    "# Anders Ågren Thuné (athune@kth.se)\n",
    "# Johan Hoffman (jhoffman@kth.se)\n",
    "\n",
    "# Code written by Anders Ågren Thuné based on the template by Johan Hoffman.\n",
    "\n",
    "# This file is part of the course DD2363 Methods in Scientific Computing\n",
    "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
    "#\n",
    "# This is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Lesser General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "28xLGz8JX3Hh",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Set up environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "colab": null,
    "colab_type": "code",
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "id": "Xw7VlErAX7NS",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "gnO3lhAigLev",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "l5zMzgPlRAF6",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Optimization is the study of finding the optimal value of some domain,\n",
    "typically the maximum or minimum of some function. This is a field\n",
    "with a broad range of applications, not least in the rapidly growing\n",
    "field of machine learning.\n",
    "\n",
    "In order to find the critical points of a function, the derivative\n",
    "or multivariate versions thereof are typically powerful tools, as they\n",
    "indicate the direction to move in the input space to achieve the largest\n",
    "change in the output, the 'steepest slope' in a sense.\n",
    "\n",
    "Two methods making use of this fact to perform unconstrained\n",
    "optimization in $\\mathbb{R}^n$ are the gradient descent method and the\n",
    "Newton method. The gradient descent method is typically used when\n",
    "training artificial neural networks, as it is a relatively cheap way\n",
    "to find minima of a cost function, especially when a stochastic\n",
    "version is used.\n",
    "\n",
    "[Hoffman 2019, Chapter 19]\n",
    "\n",
    "This report presents how the gradient descent and Newton methods were\n",
    "implemented in the Julia language. In addition, a simple feedforward\n",
    "neural network was implemented and trained using the backpropagation\n",
    "algorith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "WeFO9QMeUOAu",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### **Gradient descent and Newton's method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Gradient descent and Newton's method are very similar in that they\n",
    "utilize the fact that the gradient indicates the direction of steepest\n",
    "descent to step through the input space to search for a\n",
    "minimum. However, while gradient descent uses a step size $\\alpha^{k}$\n",
    "determined somewhat heuristically, Newton's method uses the Hessian,\n",
    "resulting in a somewhat more expensive method, but with a higher order\n",
    "of convergence.\n",
    "\n",
    "Gradient descent: $$x^{(k+1)} = x^{(k)} - \\alpha^{(k)}\\nabla\n",
    "f(x^{(k)}),$$ $$\\text{for $\\alpha^{(k)}$ so that}\\ f(x^{(k+1)}) \\leq\n",
    "\\beta f(x^{(k)}), \\quad \\text{where}\\ \\beta < 1\\ \\text{is a fixed\n",
    "parameter}$$\n",
    "\n",
    "Newton's method: $$x^{(k+1)} = x^{(k)} - (Hf(x^{(k)}))^{-1}\\nabla\n",
    "f(x^{(k)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate J(f)(p), modifying `jac` in place. Reused from lab 5.\n",
    "\"\"\"\n",
    "function filljacobian!(jac, f, p, h = 1e-5)\n",
    "    p1 = p\n",
    "    p2 = copy(p1)\n",
    "    for i in 1:length(p)\n",
    "        p1[i] += h\n",
    "        p2[i] -= h\n",
    "\n",
    "        jac[:,i] .= (f(p1) - f(p2)) / 2h\n",
    "\n",
    "        p1[i] -= h\n",
    "        p2[i] += h\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Calculate ∇(f)(x), modifying v in place.\n",
    "\"\"\"\n",
    "fillgradient!(v, f, p, h = 1e-5) = filljacobian!(v', f, p, h)\n",
    "\n",
    "\"\"\"\n",
    "Use gradient descent to find x* such that ||∇(f)(x*)|| < TOL\n",
    "\"\"\"\n",
    "function gradient_descent(f, β, x⁽⁰⁾, TOL = 1e-6)\n",
    "    x = x⁽⁰⁾\n",
    "    ∇f = Inf*ones(length(x⁽⁰⁾))\n",
    "    while norm(∇f) >= TOL\n",
    "        fillgradient!(∇f, f, x)\n",
    "        α = find_α(f, β, x, ∇f)\n",
    "        x .-= α .* ∇f\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Use Newton's method to find x* such that ||∇(f)(x*)|| < TOL\n",
    "\"\"\"\n",
    "function newton_optimization(f, H, x⁽⁰⁾, TOL = 1e-6)\n",
    "    x = x⁽⁰⁾\n",
    "    ∇f = Inf*ones(length(x⁽⁰⁾))\n",
    "    while norm(∇f) >= TOL\n",
    "        fillgradient!(∇f, f, x)\n",
    "        x .-= H(x) \\ ∇f\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "_4GLBv0zWr7m",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "ein.tags": "worksheet-0",
    "id": "6bcsDSoRXHZe",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Hoffman, J. 2019. *Introduction to Scientific Computing*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "template-report-lab-X.ipynb",
   "provenance": null,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  },
  "name": "andersthune_lab7.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
