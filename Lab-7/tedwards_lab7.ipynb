{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "template-report-lab-X.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT19/blob/tobzed/Lab-7/tedwards_lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Lab 7: Optimization and learning**\n",
        "**Tobias Edwards**\n",
        "9th of March 2019\n"
      ]
    },
    {
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The focus of this lab was function minimization. "
      ]
    },
    {
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**About the code**"
      ]
    },
    {
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "1e1de3c9-fbe5-46c9-c3de-2986013df3c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Code by Tobias Edwards (tedwards@kth.se)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "from math import *\n",
        "import numpy as np\n",
        "import unittest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given an objective function $f:D \\rightarrow R$, where $D$ is some predefined domain, we seek an element in $x^* \\in D$ such that \n",
        "$f(x^*) \\leq f(x) \\quad \\forall x \\in D$. The motivation for the methods that I have implemented below is that a minimum is either located on a domain boundary \\textit{or} where $\\nabla f = 0$. The methods follow gradients and stop when the gradient is approximately $0$.\n",
        "\n",
        "One way of approximating the minimum of $f$ is by the gradient descent method. This method, as per the name, iteratively steps along the negative gradient. \n",
        "\n",
        "Another method is the familiar Newton's method. However, instead of approximating roots for $f$ using the Jacobian $J(f)$, we approximate roots of $\\nabla f$ replacing $J(f)$ with the Hessian of $f$ defined as $H(f) = J(\\nabla f)$, i.e., $H$ is a matrix containing second partial derivatives of $f$. \n"
      ]
    },
    {
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Results**\n"
      ]
    },
    {
      "metadata": {
        "id": "1jyFWe0Zg-WE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Assignment 1. Gradient Descent\n",
        "\n",
        "Note that I used the Barzilai & Borwein method to determine how each step size should be. The method is explained in detail [here](http://pages.cs.wisc.edu/~swright/726/handouts/barzilai-borwein.pdf).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6b7XebAOhGm_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def barzilai_borwein(x_prev,x_curr,grad_f,alpha,beta):\n",
        "    delta_grad = grad_f(x_curr) - grad_f(x_prev)\n",
        "    delta_x = x_curr - x_prev\n",
        "    denom = delta_grad.dot(delta_grad)\n",
        "    nom = delta_grad.dot(delta_x)\n",
        "    if denom == 0:\n",
        "        return alpha*beta\n",
        "    return nom/denom\n",
        "\n",
        "def gradient_descent(f,grad_f,x_current,TOL):\n",
        "    alpha = 1 # initial stepsize\n",
        "    beta = 0.9\n",
        "    max_iter = 1000\n",
        "    iter = 0\n",
        "    while np.linalg.norm(grad_f(x_current)) >= TOL and iter < max_iter:\n",
        "        old_x = x_current\n",
        "        x_current = old_x - alpha*grad_f(old_x)\n",
        "        alpha = barzilai_borwein(old_x,x_current,grad_f,alpha,beta)\n",
        "        iter += 1\n",
        "    return x_current"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XJF1TKbiZvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Assignment 2. Newton's Method"
      ]
    },
    {
      "metadata": {
        "id": "TzoTQWKcicnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def newtons_method(f,grad_f,Hf,x_curr,TOL):\n",
        "    while np.linalg.norm(grad_f(x_curr)) >= TOL:\n",
        "        x_delta = np.linalg.solve(Hf(x_curr),-grad_f(x_curr))\n",
        "        x_curr += x_delta\n",
        "    return x_curr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJBtbA3cMSGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tests\n"
      ]
    },
    {
      "metadata": {
        "id": "N7DRgZU2MXzM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Lab7FunctionsTest(unittest.TestCase):\n",
        "\n",
        "    def test_gradient_descent(self):\n",
        "        f = lambda x: (x[0]-1.0)**2 + (x[1]+2.0)**2 - 3.0\n",
        "        # minimum of f is at (1,-2) and f(1,-2) = -3\n",
        "        grad_f = lambda x: np.array([2.0*(x[0]-1.0), 2.0*(x[1]+2.0)])\n",
        "        TOL_list = [.1,.01,.001,.0001,.00001,.000001,.0000001]\n",
        "        rel_error = []\n",
        "        exact = np.array([1.0,-2.0])\n",
        "        for TOL in TOL_list:\n",
        "            x_curr = gradient_descent(f,grad_f,np.array([10.0,3.0]),TOL)\n",
        "            rel_error.append(np.linalg.norm(x_curr-exact)/np.linalg.norm(exact))\n",
        "        print(\"Results for Gradient Descent\")\n",
        "        print (\"The relative error for step sizes:\")\n",
        "        for i in range(len(TOL_list)):\n",
        "            print(\"Step size: %f | Error: %f\" %(TOL_list[i],rel_error[i]))\n",
        "\n",
        "    def test_newtons_method(self):\n",
        "        f = lambda x: (x[0]-2.0)**2 + (x[1]+1.0)**2 -3\n",
        "        grad_f = lambda x: np.array([2.*(x[0]-2.0),2.0*(x[1]+1.0)])\n",
        "        Hf = lambda x: np.array([\n",
        "                    [2.0, 0.0],\n",
        "                    [0.0, 2.0]\n",
        "                ])\n",
        "        TOL_list = [.1,.01,.001,.0001,.00001,.000001,.0000001]\n",
        "        rel_error = []\n",
        "        exact = np.array([2.0,-1.0])\n",
        "        for TOL in TOL_list:\n",
        "            x_curr = newtons_method(f,grad_f,Hf,np.array([10.0,3.0]),TOL)\n",
        "            rel_error.append(np.linalg.norm(x_curr-exact)/np.linalg.norm(exact))\n",
        "        print(\"Results for Newton's method\")\n",
        "        print (\"The relative error for step sizes:\")\n",
        "        for i in range(len(TOL_list)):\n",
        "            print(\"Step size: %f | Error: %f\" %(TOL_list[i],rel_error[i]))\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I find that understanding the method and how it is derivied mathematically is easy enough to understand. However, I have difficulty in getting intuition for what is going on.  Also, I find that testing has become quite difficult when intuitively I can't entirely see what/how to test. Currently I plot the graphs of my solutions against exact solutions and examine the difference. For instance i tried to do a squared error method but this resulted in nothing.  I couldnt tell if this was because the error was so small that it was pratically 0 or if the approixmation was giving exact points. "
      ]
    }
  ]
}