{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "template-report-lab-X.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363-VT19/blob/bozzato/Lab-2/bozzato_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6RgtXlfYO_i7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Lab 2: Direct methods**\n",
        "**Bozzato Federico**"
      ]
    },
    {
      "metadata": {
        "id": "9x_J5FVuPzbm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "metadata": {
        "id": "6UFTSzW7P8kL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this lab we will see how these general methods can be used to solve algebraic problems, such as to find the inverse of a matrix or to compute the eigenvalues of a certain matrix, which are theoretically simple to solve but very hard to compute."
      ]
    },
    {
      "metadata": {
        "id": "OkT8J7uOWpT3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**About the code**"
      ]
    },
    {
      "metadata": {
        "id": "HmB2noTr1Oyo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Author:** Federico Bozzato "
      ]
    },
    {
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab_type": "code",
        "outputId": "cc3195b1-c002-4d2b-af18-c0bb2bea171e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2019 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "28xLGz8JX3Hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "metadata": {
        "id": "D2PYNusD08Wa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "metadata": {
        "id": "Xw7VlErAX7NS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import sys\n",
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnO3lhAigLev",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "metadata": {
        "id": "l5zMzgPlRAF6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Direct methods are general methods for constructing a proof of the existence of a minimizer for a given functional [1]. This kind of methods are widely used to solve several algebraic problems, which may be computationally difficult to implement: for example to find eigenvalues and eigenvectors of an $n\\times n$ matrix with $n \\ge 4$ is a non trivial problem to solve.\n",
        "\n",
        "Even though some of these methods can give us approximate results, for particular types of inputs these methods reach high performances. In fact, in addition to being used to prove the existence of a solution,  direct methods may be used to compute the solution to derised accurancy [1].\n",
        "\n",
        "\n",
        "This report is divided into three parts:\n",
        "\n",
        "1. Methods: in this section, each algorithm is presented and explained, giving also the mathematic definition of the operation the algorithm implements\n",
        "\n",
        "2. Results: in this section, the results of each algorithm are presented.\n",
        "\n",
        "3. Discussion: in this last section, results are discussed."
      ]
    },
    {
      "metadata": {
        "id": "WeFO9QMeUOAu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Methods**"
      ]
    },
    {
      "metadata": {
        "id": "zF4iBj5VURZx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This section is divided into two subsection: the mandatory part and the extra assignment."
      ]
    },
    {
      "metadata": {
        "id": "xLMPBDX9zZv4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mandatory assignment"
      ]
    },
    {
      "metadata": {
        "id": "4A93KxfLzI_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###1. QR factorization\n",
        "####Definition\n",
        "QR factorization is a method used to solve a generic equation such as $Ax=b$ where $A$ is an $n\\times n$ matrix, $b$ is an $n\\times 1$ vector and $x$ is the unknown vector. \n",
        "\n",
        "The key idea of QR factorization consists of decomposing an $n\\times n$ matrix $A$ into the product $A=QR$, where $Q$ is an orthogonal matrix (which means $Q^{-1}=Q^{T}$) and $R$ is an upper triangular matrix [2].\n",
        "\n",
        "####Implementation\n",
        "One way to implement QR factorization is to start from Gram-Schmidt algorithm, that orthogonalizes vectors belonging to the same vector space $V$. \n",
        "\n",
        "Given a square matrix $A\\in\\mathbb{R}^{n\\times n}$, with its columns defining the vectors $\\{a_{i}\\}_{i=1}^n$, the new orthogonal vectors are given by [3]\n",
        "\n",
        ">$\n",
        "v_j = a_j - \\displaystyle \\sum_{i=1}^{j-1} \\left(a_j,q_i\\right)q_i \\qquad \\forall j=1,\\dots,n\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "* $a_j$ is the $j-$th column of matrix $A$\n",
        "* $q_i$ is the normalized vector $q_j= \\frac{v_i}{||v_i||}$\n",
        "\n",
        "These equations can be rewritten in the following way\n",
        "\n",
        ">$\n",
        "  \\begin{align*}\n",
        "    a_1 &= r_{11}q_1 \\\\\n",
        "    a_2 &= r_{12}q_1 + r_{22}q_2 \\\\\n",
        "           &\\vdots\\\\\n",
        "    a_n &= r_{1n}q_1 + \\dots + r_{nn}q_n\n",
        "  \\end{align*}\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "* $r_{ij}=(a_j,q_i)$\n",
        "* $r_{jj}=||a_j-\\sum_{i=1}^{j-1}(a_j,q_i)q_i||$\n",
        "\n",
        "As it is possible to observe, the rewritten equations corresponds to product $A=QR$, where the columns of $Q$ are given by ${q_i}_{i=1}^n$ and $R$ is an uppen triangular matrix.\n",
        "\n",
        "The algorithm implementing the QR factorization is the following one [4]:\n",
        "\n",
        "\n",
        "```\n",
        "for i = 1 to n do\n",
        "  v_i = a_i\n",
        "  r_ii = abs(v_i)\n",
        "  q_i= v_i / r_ii\n",
        "  for j = i+1 to n do\n",
        "    r_ij = transpose(q_i)v_j\n",
        "    v_j = v_i - r_ij*q_i\n",
        "  end\n",
        "end\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qRzUmBZYzzvC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def QRfactorization(matrixA):\n",
        "\t'''\n",
        "\tCalculates the QR factorization of matrixA which must be invertible\n",
        "\tParameters:\n",
        "\t- matrixA: square matrix to be factorized and returns the orthogonal matrix Q and \n",
        "\tthe upper triangular matrix R such that A= QR\n",
        "\tOutput:\n",
        "\t- Q: orthogonal matrix\n",
        "\t- R: upper triangular matrix \n",
        "\t'''\n",
        "\tif not isinstance(matrixA,np.ndarray):\n",
        "\t\tmatrixA= np.array(matrixA)\n",
        "\t\n",
        "\t\n",
        "\tif matrix_rank(matrixA) != matrixA.shape[1]:\n",
        "\t\tprint('Error: matrix determinat is equal to 0. Impossible to do the',\n",
        "\t\t\t'factorization!')\n",
        "\t\tsys.exit(1)\t\n",
        "\t\t\n",
        "\tif matrixA.shape[0] != matrixA.shape[1]:\n",
        "\t\tprint('Only square matrices are allowed!')\n",
        "\t\tsys.exit(1)\n",
        "\t\n",
        "\tn= matrixA.shape[0]\n",
        "\t\n",
        "\tv= np.zeros((n,n))\n",
        "\tfor i in range(0,n):\n",
        "\t\tv[:,i]= matrixA[:,i]\n",
        "\n",
        "\tQ= np.zeros((n,n))\n",
        "\tR= np.zeros((n,n))\n",
        "\tfor i in range(0,n):\n",
        "\t\tr_ii= np.dot(v[:,i],v[:,i])**(0.5)\n",
        "\t\tR[i,i]= r_ii\n",
        "\t\tq_i= v[:,i]/r_ii\n",
        "\t\tQ[:,i]= q_i\n",
        "\t\t\n",
        "\t\tfor j in range(i+1,n):\n",
        "\t\t\tr_ij= np.dot(q_i,v[:,j])\n",
        "\t\t\tR[i,j]= r_ij \n",
        "\t\t\tv[:,j] = v[:,j] - r_ij*q_i\n",
        "\t\t\t\n",
        "\treturn Q,R\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WUFjTcGDzPV0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###2. Direct solver $Ax=b$\n",
        "####Definition\n",
        "Let $A$ and $b$ be an $n\\times n$ matrix and $n\\times 1$ vector, respectively. It is possible to solve the system of equations $Ax=b$ by using the QR factorization. In fact, $A$ can be decomposed into the product $A=QR$, where $Q$ is a orthogonal matrix and $R$ is an upper triangular matrix. Thus,\n",
        "\n",
        ">$\n",
        "  Ax= B \\Longrightarrow QRx=b\n",
        "$\n",
        "\n",
        "Since $Q^{-1}= Q^{T}$ because $Q$ is orthogonal, we obtain\n",
        "\n",
        ">$\n",
        "Rx= Q^{T}b\n",
        "$\n",
        "\n",
        "Now, in order to get $x$, we can use the *backward substitution* explained in [5].\n",
        "\n",
        "\n",
        "####Implementation\n",
        "One of the possible implementations is the following one:"
      ]
    },
    {
      "metadata": {
        "id": "c01A9Jwo0CrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linearSystemSolver(matrixA,b):\n",
        "\t'''\n",
        "\tCalculates the unknown vector x of the equation Ax=b\n",
        "\t''' \n",
        "\t\n",
        "\t# Ax=b -> QRx=b -> Rx=Q.Tb -> backward substitution\n",
        "\tQ, R = QRfactorization(matrixA);\n",
        "\t\n",
        "\tif not isinstance(matrixA,np.ndarray):\n",
        "\t\tmatrixA= np.array(matrixA)\n",
        "\t\t\n",
        "\tif not isinstance(b,np.ndarray):\n",
        "\t\tb= np.array(b)\n",
        "\t\n",
        "\t\n",
        "\tnew_b= np.dot(Q.T,b)\n",
        "\t\n",
        "\tx= np.zeros((matrixA.shape[1],1))\n",
        "\tx[-1]= new_b[-1]/R[-1][-1]\n",
        "\tfor i in range(matrixA.shape[0]-2,-1,-1):\n",
        "\t\ts= 0\n",
        "\t\tfor j in range(i+1,matrixA.shape[1]):\n",
        "\t\t\ts += R[i,j]*x[j]\n",
        "\t\tx[i]= (new_b[i] - s)/R[i,i]\n",
        "\t\n",
        "\tx= x.tolist()\n",
        "\tx= [x[i][0] for i in range(0,len(x))]\n",
        "\t\n",
        "\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCl9yBB3zeNX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extra assignment"
      ]
    },
    {
      "metadata": {
        "id": "8RqiZaZ-zh4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3. Least squares problem $Ax=b$\n",
        "####Definition\n",
        "In the section about the direct solver we explored the algorithm for solving systems of linear equations $Ax=b$ where $A$ is a square $n\\times n$ matrix. Now, let us consider a system of linear equations $Ax=b$ where, instead, $A$ is a $m\\times n$ matrix, with $m>n$ and $b\\in\\mathbb{R}^m$. Since there exists no inverse matrix $A^{-1}$, we cannot use the previous algorithm to solve $Ax=b$ but we have to apply other algorithms.\n",
        "\n",
        "We will examine the general case, where $b\\notin \\text{range}(A)$: for these cases, we say that the system $Ax=b$ is *overdetermined* [6]. Since there exists no exact solution $x$ to the system $Ax=b$, we have to find an approximation of the unknown vector $x$ that solves the system, and one method is called *least squares problem*. \n",
        "\n",
        "Least squares problem consists of minimizing the residual $r= b-Ax \\in\\mathbb{R}^m$, or similarly\n",
        "\n",
        ">$\n",
        "\\displaystyle x=\\arg \\min_{y\\in\\mathbb{R}^n} ||b-Ay||\n",
        "$\n",
        "\n",
        "Since $b\\notin\\text{range}(A)$, looking for $x$ such that minimizes the residual $r=b-Ax$ means that we are seeking a vector $x\\in\\mathbb{R}^n$ such that the Euclidian distance betweem $Ax$ and $b$ is minimal [6]: this is equal to\n",
        "\n",
        ">$\n",
        "A^Tr= 0 \\Longleftrightarrow A^TAx=A^Tb\n",
        "$\n",
        "\n",
        "And thus, $A^TA$ is always symmetric positive-semidefinite matrix because it has only real entries [7]. If $A^TA$ is invertible, the vector $x$ is given by\n",
        "\n",
        ">$\n",
        "  x= \\left(A^TA\\right)^{-1}A^Tb\n",
        "$\n",
        "\n",
        "####Implementation\n",
        "For the implementation of the least squares problem, we start considering the equation\n",
        "\n",
        ">$\n",
        "  A^TAx= A^Tb\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "* $D=A^TA\\in\\mathbb{R}^{n\\times n}$\n",
        "* $x\\in\\mathbb{R}^n$\n",
        "* $e=A^Tb\\in\\mathbb{R}^n$\n",
        "\n",
        "Consequently, we can rewrite the equation $A^TAx= A^Tb$ as\n",
        "\n",
        ">$\n",
        "Dx=e\n",
        "$\n",
        "\n",
        "and use the direct solver algorithm presented in the previous section."
      ]
    },
    {
      "metadata": {
        "id": "q4g1hpwi0S3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def leastSquareProblem(matrixA,b):\n",
        "\tif not isinstance(matrixA,np.ndarray):\n",
        "\t\tmatrixA= np.array(matrixA)\n",
        "\t\n",
        "\tif matrixA.shape[0] < matrixA.shape[1]:\n",
        "\t\tprint('Too many unknowns: impossible to solve the system')\n",
        "\t\tsys.exit(1)\n",
        "\t\t\n",
        "\t# problem: from Ax=b to x=(A.tA)^-1A.tb\n",
        "\t# first step: A.tAx= A.tb and then use linearSystemSolver\n",
        "\t\n",
        "\tA= np.dot(matrixA.T,matrixA)\n",
        "\tnew_b= np.dot(matrixA.T,b)\n",
        "\t\n",
        "\tx= linearSystemSolver(A,new_b)\n",
        "\t\t\n",
        "\treturn x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iCs7mXq8zoaI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###4. QR eigenvalue algorithm\n",
        "####Definition\n",
        "The eigenvalues for a matrix $A\\in\\mathbb{R}^{n\\times n}$ are defined as the solution of the equation\n",
        "\n",
        ">$\n",
        "\\det(A-\\lambda I)=0\n",
        "$\n",
        "\n",
        "which is known as *characteristic equation* for $A$.\n",
        "\n",
        "The eigenvectors $\\{x_i\\}$ associated to the $i-$th eigenvalue $\\lambda_i$ are defined by \n",
        "\n",
        ">$\n",
        "  \\left(A-\\lambda_iI\\right)x=0\n",
        "$\n",
        "\n",
        "\n",
        "####Implementation\n",
        "One way to seek the eigenvalues and their corresponding eigenvectors for a square $n\\times n$ matrix $A$ is to find a matrix $T$ which is similar to $A$ by using similarity transforms. In fact, similar matrices have the same eigenvalues [8,9,10,11].\n",
        "\n",
        "My implementation consists of two parts:\n",
        "\n",
        "1. QR algorithm [12,13]: this algorithm is based on Schur factorization of a matrix from successive QR factorizations."
      ]
    },
    {
      "metadata": {
        "id": "y3D6_m3U0WP8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eigenvaluesQRalgorithm(matrixA,converge=1e-8):\n",
        "\tif not isinstance(matrixA,np.ndarray):\n",
        "\t\tmatrixA= np.array(matrixA)\n",
        "  \n",
        "  # QR algorithm\n",
        "\tn= matrixA.shape[0]\n",
        "\tA= matrixA\n",
        "\tit= 1\n",
        "\tQ= []\n",
        "\twhile it < 2500:\n",
        "\t\tQ_tmp,R= QRfactorization(A)\n",
        "\t\tA= np.dot(R,Q_tmp)\n",
        "\t\tif it == 1:\n",
        "\t\t\tQ= Q_tmp\n",
        "\t\telse:\n",
        "\t\t\tQ= np.dot(Q,Q_tmp)\n",
        "\t\tit += 1 \n",
        "\t\n",
        "\teigenval= [A[i,i] for i in range(0,n)]\n",
        "\teigenvec= []\n",
        "\t\n",
        "  # Rayleigh quotation\n",
        "\tfor i in range(0,len(eigenval)):\n",
        "\t\tb = Q[:,i]\n",
        "\t\tmu= eigenval[i]\n",
        "\t\tmatr= matrixA - mu*np.eye(n)\n",
        "\t\t\n",
        "\t\tr= np.dot(matrixA,b) - mu*b\n",
        "\t\tr= np.inner(r,r)**(0.5)\n",
        "\t\t\n",
        "\t\titerations= 1\n",
        "\t\twhile r > converge and iterations < 1000:\n",
        "\t\t\ttmp= linearSystemSolver(matr,b) \n",
        "\t\t\tif math.isnan(np.inner(tmp,tmp)) or np.inner(tmp,tmp) == 0:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tb= tmp / np.inner(tmp,tmp)\n",
        "\t\t\tmu= np.dot(np.dot(b.T,matrixA),b)/np.inner(b,b)\n",
        "\t\t\tr= np.dot(matrixA,b) - mu*b\n",
        "\t\t\tr= np.inner(r,r)**(0.5)\n",
        "\t\t\tprint('r=', r)\n",
        "\t\t\titerations += 1\n",
        "\t\t\n",
        "\t\teigenval[i]= mu\n",
        "\t\tb= b / (np.inner(b,b)**(0.5))\n",
        "\t\teigenvec.append(b)\n",
        "\t\n",
        "\treturn eigenval, eigenvec\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SsQLT38gVbn_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ]
    },
    {
      "metadata": {
        "id": "RLwlnOzuV-Cd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Present the results. If the result is an algorithm that you have described under the *Methods* section, you can present the data from verification and performance tests in this section. If the result is the output from a computational experiment this is where you present a selection of that data. "
      ]
    },
    {
      "metadata": {
        "id": "_4GLBv0zWr7m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "metadata": {
        "id": "6bcsDSoRXHZe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Summarize your results and your conclusions. Were the results expected or surprising. Do your results have implications outside the particular problem investigated in this report? "
      ]
    },
    {
      "metadata": {
        "id": "1vcIILuQYsEA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **References**\n",
        "\n",
        "[1] from Wikipedia, *[Direct methods](https://en.wikipedia.org/wiki/Direct_method_in_the_calculus_of_variations)*\n",
        "\n",
        "[2] from Wikipedia, *[QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition)*\n",
        "\n",
        "[3] from Lecture Notes, *Classical Gram-Schmidt orthogonalization, chapter 5*, pg. 69\n",
        "\n",
        "[4] from Lecture Notes, *Modified Gram-Schmidt orthogonalization, chapter 5*, pg. 70\n",
        "\n",
        "[5] from Lecture Notes, *Triangular matrices, chapter 5*, pg. 69\n",
        "\n",
        "[6] from Lecture Notes, *Least square problems, chapter 5*, pg. 74\n",
        "\n",
        "[7] from Wikipedia, *[Transpose](https://en.wikipedia.org/wiki/Transpose)*\n",
        "\n",
        "[8] from Lecture Notes, *Theorem 7, chapter 6*, pg. 80\n",
        "\n",
        "[9] from Lecture Notes, *Theorem 8, chapter 6,* pg. 81\n",
        "\n",
        "[10] from Lecture Notes, *Theorem 9, chapter 6,* pg. 81\n",
        "\n",
        "[11] from Lecture Notes, *Theorem 10, chapter 6*, pg. 81\n",
        "\n",
        "[12] from YouTube, *[QR algorithm for eigenvalues](https://www.youtube.com/watch?v=_neGVEBjLJA)*\n",
        "\n",
        "[13] from Lecture Note, *QR algoritm, chapter 6*, pg. 82"
      ]
    }
  ]
}